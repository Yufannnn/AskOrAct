# Ask or Act for Cooperative Assistance via Inverse Planning: Final Report

## Introduction
Humans helping each other under ambiguity face a recurring choice: act immediately based on current belief, or ask a clarifying question first. This project studies that tradeoff in a controlled cooperative setting where a principal has a hidden goal and an assistant must help complete it.

The central research question is:

- Under instruction ambiguity and hidden goals, can an inverse-planning assistant that optimizes value of information reduce team cost and improve task outcomes compared with assistants that never ask, always ask, or follow instruction literally?

We frame this as a sequential decision problem with uncertainty over goals. The assistant observes:

- current world state,
- an ambiguous templated instruction,
- principal behavior generated by an approximately rational policy.

At each step, the assistant decides between:

- `ask`: spend one interaction to query a fixed clarification question,
- `act`: execute a task action toward a candidate goal.

The final system is fully reproducible with fixed seeds, deterministic sweep configuration, and report/plot generation scripts.

## Background
The project combines four ideas:

1. Inverse planning:
   infer latent goals from observed behavior under a noisy-rational action model.
2. Bayesian belief tracking:
   maintain a posterior over candidate goals and update as new evidence arrives.
3. Value of information:
   ask only when expected information gain outweighs communication/time cost.
4. Cooperative assistance:
   optimize assistant behavior for joint outcome under uncertainty, not just local immediate action.

This setup intentionally avoids large language models. Language is a controlled symbolic channel:

- instruction templates map to candidate goal sets,
- question menu is fixed,
- answer model is fixed and probabilistic.

This keeps the study focused on decision-theoretic ask-versus-act behavior instead of open-domain NLP.

## Method Overview
### Environment and goal semantics
The world is a small gridworld (`N=9`, `M=6` objects by default) with two agents:

- principal: reveals intent indirectly via movement policy,
- assistant: must pick the true goal object.

Important semantics:

- principal cannot complete the task by picking (`PRINCIPAL_CAN_PICK=False`),
- success requires assistant picking the true object,
- wrong picks can be configured as recoverable or terminal (ablation switch),
- asking may consume time depending on mode.

### Principal model
Principal policy is approximately rational with parameters:

- `beta` (rationality concentration) in `{1.0, 2.0, 4.0}`,
- `eps` (action noise) in `{0.0, 0.05, 0.1}`.

Higher `beta` sharpens goal-directed behavior; higher `eps` injects randomness.

### Belief inference
The assistant maintains a posterior over candidate goals and updates from:

- instruction-conditioned candidate set,
- principal actions via inverse planning likelihood,
- optional question answers via answer likelihood model.

Additional refinements included:

- posterior normalization fixes in expected answer value computation,
- posterior elimination after wrong assistant pick for recoverable episodes,
- no repeated question in an episode.

### Ask-or-act decision rule
For each step, the assistant compares expected team cost:

- `CostAct`: act now using current posterior,
- `CostAsk`: ask best available question, update posterior, then act.

Key safeguards to prevent pathological asking:

- entropy gate (`ENTROPY_GATE`),
- ask window (`ASK_WINDOW`),
- max question cap (`MAX_QUESTIONS` and optional per-episode cap),
- strict improvement criterion (`ask` only if expected cost is lower).

### Additional baseline policies
Beyond `ask_or_act`, `never_ask`, and `always_ask`, we evaluate two clarification baselines:

- `info_gain_ask`: computes question-level expected entropy reduction
  `IG(q)=H(b)-E[H|q]`, asks `argmax_q IG(q)` when gating conditions pass, otherwise acts on MAP goal.
- `random_ask`: uses the same ask gating as `info_gain_ask`, but selects a random available question when asking.

### Cost and regret
Primary cost definition:

- `team_cost = steps + QUESTION_COST * questions_asked`
- `regret = team_cost - oracle_cost`

For failed episodes:

- `team_cost = episode_max_steps + QUESTION_COST * questions_asked`

Oracle cost is computed from shortest assistant path to true goal plus pick under assistant-only completion semantics.

## Experimental Setup
## Conditions
Main sweep:

- ambiguity `K in {1,2,3,4}`,
- noise `eps in {0.0, 0.05, 0.1}`,
- rationality `beta in {1.0, 2.0, 4.0}`,
- policies `{ask_or_act, never_ask, always_ask, info_gain_ask, random_ask}`.

Replicate protocol:

- `REPL_SEEDS = [0,1,2,3,4]`,
- `N_EPISODES_PER_SEED = 20`,
- total `100` episodes per `(policy, K, eps, beta)` condition.

Deterministic per-episode seed formula:

- `env_seed = base_seed + 100000*rep + 1000*K + 100*int(eps*100) + 10*beta + episode_id`

### Hardness mechanisms
To make ask-versus-act consequential:

- dynamic per-episode deadline:
  `episode_max_steps = oracle_steps + DEADLINE_MARGIN`,
- candidate goal objects are sampled far apart (and across rooms where possible),
- asking can consume a step budget (mode-dependent).

### Evaluation metrics
Primary:

- success rate,
- regret,
- average questions asked,
- MAP goal correctness.

Diagnostic:

- failure-by-timeout rate,
- failure-by-wrong-pick rate (when enabled),
- paired policy-difference confidence intervals.

Uncertainty:

- 95% bootstrap confidence intervals over episodes (`B=1000`).

## Evaluations
### Main sweep trends
Aggregate by ambiguity `K` (averaged over `eps` and `beta`) shows expected behavior:

| K | Policy | Success | Avg Questions | Avg Regret |
|---|---|---:|---:|---:|
| 1 | ask_or_act | 100.00% | 0.00 | 0.53 |
| 1 | never_ask | 100.00% | 0.00 | 0.53 |
| 1 | always_ask | 100.00% | 0.00 | 0.53 |
| 2 | ask_or_act | 91.11% | 0.00 | 1.67 |
| 2 | never_ask | 91.11% | 0.00 | 1.67 |
| 2 | always_ask | 100.00% | 1.72 | 3.07 |
| 3 | ask_or_act | 91.67% | 0.67 | 2.14 |
| 3 | never_ask | 83.78% | 0.00 | 2.15 |
| 3 | always_ask | 90.22% | 2.29 | 4.08 |
| 4 | ask_or_act | 87.33% | 0.61 | 2.42 |
| 4 | never_ask | 78.89% | 0.00 | 2.40 |
| 4 | always_ask | 86.44% | 2.58 | 4.56 |

Interpretation:

- At low ambiguity (`K<=2`), asking is mostly unnecessary.
- At high ambiguity (`K=3,4`), `ask_or_act` asks selectively (`~0.6` questions), improving completion over `never_ask`.
- `always_ask` often over-pays communication/time cost, producing much higher regret.

![Main evaluation dashboard (success, regret, questions, MAP vs K with 95% CI)](../results/main_dashboard.png)

*Figure 1. Main evaluation dashboard across ambiguity levels.*

### Robustness with confidence intervals
For `K=3,4`, bootstrap CIs remain tight and trends are stable:

- `K=3`, success:
  - ask_or_act: `91.67% [89.78, 93.44]`
  - never_ask: `83.78% [81.22, 86.22]`
  - always_ask: `90.22% [88.33, 92.11]`
- `K=4`, success:
  - ask_or_act: `87.33% [85.11, 89.34]`
  - never_ask: `78.89% [75.89, 81.34]`
  - always_ask: `86.44% [84.00, 88.34]`

Regret CIs at high ambiguity show persistent separation:

- `K=4` regret:
  - ask_or_act: `2.42 [2.30, 2.53]`
  - always_ask: `4.56 [4.46, 4.66]`

### Policy-difference CIs (direct hypothesis test view)
Paired bootstrap deltas over matched episode keys:

| Contrast (K=3,4) | Delta mean [95% CI] | N paired episodes |
|---|---:|---:|
| DeltaSuccess (AskOrAct - NeverAsk) | 8.17% [6.61%, 9.78%] | 1800 |
| DeltaRegret (AskOrAct - AlwaysAsk) | -2.04 [-2.13, -1.95] | 1800 |
| DeltaSuccess (AskOrAct - InfoGainAsk) | -3.72% [-4.94%, -2.61%] | 1800 |
| DeltaRegret (AskOrAct - InfoGainAsk) | -0.46 [-0.53, -0.40] | 1800 |

These deltas directly support the intended claim:

- selective asking increases success versus never asking under high ambiguity,
- selective asking lowers regret versus always asking,
- and compared with `info_gain_ask`, AskOrAct trades some success for meaningfully lower regret under current cost/deadline settings.

### Clarification quality of the first asked question
Using first-ask diagnostics at `K=3,4`, we measure posterior contraction from a single query:

- AskOrAct: average `DeltaH` about `0.45`, `DeltaN_eff` about `1.00`.
- InfoGainAsk: average `DeltaH` about `0.75`, `DeltaN_eff` about `1.77`.
- RandomAsk: average `DeltaH` about `0.37`, `DeltaN_eff` about `0.87`.

This ranking is expected: entropy-targeted question selection (`info_gain_ask`) is strongest on immediate posterior contraction, while AskOrAct optimizes downstream team cost rather than information gain alone.

![Clarification quality entropy delta (K=3,4)](../results/clarification_quality_entropy_delta.png)

*Figure 2. Mean entropy reduction after the first asked question, with bootstrap uncertainty.*

### Failure-mode analysis
In the main sweep (`WRONG_PICK_FAIL=False`), failures are timeout-driven:

- `K=3` failure rates:
  - ask_or_act `8.33%` (timeout `8.33%`, wrong-pick `0.00%`)
  - never_ask `16.22%` (timeout `16.22%`, wrong-pick `0.00%`)
  - always_ask `9.78%` (timeout `9.78%`, wrong-pick `0.00%`)
- `K=4` failure rates:
  - ask_or_act `12.67%` (timeout `12.67%`, wrong-pick `0.00%`)
  - never_ask `21.11%` (timeout `21.11%`, wrong-pick `0.00%`)
  - always_ask `13.56%` (timeout `13.56%`, wrong-pick `0.00%`)

This is consistent with deadline-based hardness: failures are mainly from wasted steps rather than terminal wrong picks in the default setting.

### Cost-model ablations
We separate time cost and communication cost:

- Mode A (time-only): ask consumes step, `QUESTION_COST=0.0`
- Mode B (comm-only): ask does not consume step, `QUESTION_COST=0.5`
- Mode C (both): ask consumes step, `QUESTION_COST=0.5`

High-ambiguity (`K=3,4`, wrong-pick-fail disabled) summaries:

- Mode A:
  - ask_or_act success `89.58%`, regret `1.72`
  - never_ask success `85.42%`, regret `2.16`
  - always_ask success `81.25%`, regret `2.85`
- Mode B:
  - ask_or_act success `90.97%`, regret `1.66`
  - never_ask success `82.64%`, regret `2.22`
  - always_ask success `95.83%`, regret `2.03`
- Mode C:
  - ask_or_act success `87.50%`, regret `2.37`
  - never_ask success `84.03%`, regret `1.98`
  - always_ask success `78.47%`, regret `4.36`

Interpretation:

- If asking has only communication cost (Mode B), frequent asking can maximize success.
- If asking burns deadline budget (Mode A/C), over-asking is harmful.
- AskOrAct remains the most balanced strategy across regimes.

![Ablation dashboard across mode and wrong-pick-fail settings](../results/ablations_dashboard.png)

*Figure 3. Ablation dashboard showing sensitivity to cost semantics and failure hardness.*

### Focused robustness sweeps (K=3,4)
We ran two targeted robustness evaluations with replicate seeds and paired-bootstrap delta analysis:

1. Answer-noise robustness:
   `answer_noise in {0.0, 0.1, 0.2}`.
2. Model-mismatch robustness:
   principal `beta in {1.0, 2.0, 4.0}` with assistant `beta` fixed.

Summary:

- AskOrAct retains positive `DeltaSuccess` vs NeverAsk across tested answer-noise levels.
- AskOrAct retains strongly negative `DeltaRegret` vs AlwaysAsk across both robustness sweeps.
- Under principal-model mismatch, AskOrAct remains competitive on success while consistently improving regret.

![Answer-noise robustness deltas](../results/robust_answer_noise_deltas.png)

*Figure 4. Paired delta success/regret under increasing answer noise.*

![Principal-model mismatch robustness deltas](../results/robust_mismatch_deltas.png)

*Figure 5. Paired delta success/regret under principal rationality mismatch.*

## Discussion
### What worked
- Inverse-planning posterior updates are informative enough to reduce unnecessary questions at low ambiguity.
- Entropy-gated ask policy reliably activates at high ambiguity.
- Deadline semantics and far-apart candidates produce meaningful policy separation in success and regret.
- Replicate-seed uncertainty reporting supports robust claims instead of single-run anecdotes.

### Limitations
- Language channel is templated and symbolic; findings do not yet cover open-ended natural language.
- Environment remains small and discrete.
- Principal policy class is hand-specified noisy-rational behavior.
- Wrong-pick terminal semantics are ablated but not default in main comparison for backward comparability.

### Future extensions
- Larger maps and richer object/task compositions.
- Adaptive question menu learning rather than fixed handcrafted question sets.
- Richer principal behavior models and mis-specification robustness tests.
- Cross-domain transfer tests to assess whether ask policy generalizes beyond current templates.

## Conclusion
This project demonstrates that a selective ask-or-act assistant can improve cooperative performance under ambiguity without relying on large NLP models. In the high-ambiguity regime (`K=3,4`), AskOrAct improves success over NeverAsk while substantially reducing regret relative to AlwaysAsk. Confidence intervals and paired delta analyses support that these differences are systematic, not isolated to specific seeds.

The implementation now includes:

- reproducible sweeps with replicate seeds,
- uncertainty-aware evaluation,
- ablation modes separating time and communication costs,
- explicit failure-mode reporting,
- end-to-end packaging and presentation artifacts.

Overall, the final system supports the original thesis: asking should be treated as a strategic cooperative action whose value depends on uncertainty, deadline pressure, and question cost.

## Artifact Index
- Main report tables and condition-level metrics: `results/full_report.md`
- Main dashboard figure: `results/main_dashboard.png`
- Clarification quality figure: `results/clarification_quality_entropy_delta.png`
- Ablation dashboard figure: `results/ablations_dashboard.png`
- Robustness figures: `results/robust_answer_noise_deltas.png`, `results/robust_mismatch_deltas.png`
- Main episode-level data: `results/metrics.csv`
- Ablation episode-level data: `results/metrics_ablations.csv`
- Robustness data: `results/metrics_robust_answer_noise.csv`, `results/metrics_robust_mismatch.csv`
